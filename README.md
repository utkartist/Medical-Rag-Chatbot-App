# Bio Medical RAG App



https://github.com/user-attachments/assets/3c06c147-5a91-4d8a-ba74-f0a617da9466




## Table of Contents
- [Overview](#overview)
- [Technology Stack](#technology-stack)
  - [BioMistral 7B](#biomistral-7b)
  - [PubMedBert](#pubmedbert)
  - [Qdrant](#qdrant)
  - [Langchain & Llama CPP](#langchain--llama-cpp)
- [How It Works](#how-it-works)
  - [Document Embedding](#document-embedding)
  - [Vector Storage and Search](#vector-storage-and-search)
  - [Contextual Response Generation](#contextual-response-generation)
  - [Orchestration](#orchestration)
- [Use Case](#use-case)
- [Conclusion](#conclusion)


![Screenshot 2024-08-10 032509](https://github.com/user-attachments/assets/2798f8e6-99b6-4373-ae52-1c44a73098ed)



## Overview
The Bio Medical RAG App is an advanced application designed to implement Retrieval-Augmented Generation (RAG) in the biomedical field. The application utilizes two specific PDF documents to retrieve and generate contextually relevant information, offering a powerful tool for information retrieval in medical and research settings.

## Technology Stack
The project employs a robust open-source technology stack to achieve high accuracy and efficiency in information retrieval and generation:

### BioMistral 7B
BioMistral 7B is the primary language model used in this project. It has been fine-tuned for the biomedical domain, enabling it to effectively understand and generate complex biomedical text.

### PubMedBert
PubMedBert serves as the embedding model, providing high-quality embeddings specifically trained on biomedical and clinical literature. These embeddings are crucial for accurately retrieving relevant information from the document set.

### Qdrant
Qdrant is utilized as a self-hosted vector database, which stores the embeddings generated by PubMedBert. It facilitates fast and accurate vector searches, allowing for the quick retrieval of relevant document sections based on user queries.

### Langchain & Llama CPP
Langchain and Llama CPP are used for orchestrating the various components of the application. Langchain provides a flexible framework for working with large language models, while Llama CPP ensures efficient execution, particularly when running on local systems.

## How It Works
The Bio Medical RAG App operates through a series of steps designed to retrieve and generate accurate, contextually relevant responses:

### Document Embedding
The two PDF documents are processed using PubMedBert to generate embeddings. These embeddings capture the semantic content of the text, which is essential for effective information retrieval.

### Vector Storage and Search
The generated embeddings are stored in the Qdrant vector database. When a user submits a query, Qdrant performs a vector search to find the most relevant embeddings, corresponding to the sections of the documents that are likely to contain the information needed.

### Contextual Response Generation
The relevant document sections retrieved from Qdrant are then passed to BioMistral 7B. The model uses this context to generate a detailed and accurate response that directly addresses the user's query.

### Orchestration
The orchestration of these processes is handled by Langchain and Llama CPP, ensuring that the workflow is efficient and scalable.

## Use Case
This project is ideal for applications requiring precise and contextually rich information retrieval from biomedical datasets. It can be applied in various domains, including medical research, clinical decision support, and educational tools for healthcare professionals.

## Conclusion
The Bio Medical RAG App demonstrates the effectiveness of combining advanced language models with specialized embeddings and efficient vector search technologies. It offers a powerful tool for enhancing information retrieval and generation in the biomedical field.

